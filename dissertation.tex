%-----------------------------------------------------------------------
% Latex Thesis/Dissertation Template for Wright State University
%
% Written by Sean A. Mortara
% 28 June 2001
% Modified by Josh Mark
% 15 Dec 2011
% Later edits by Joseph C. Slater
%-----------------------------------------------------------------------
\documentclass[12pt]{report}

\usepackage{xcolor}
\usepackage{booktabs} % used for tables
\usepackage{multirow} % used for tables to merge multiple rows
\usepackage{bigdelim} % used for tables to set spacing
\usepackage{bigstrut} % used for tables to set spacing
\usepackage{graphicx} % used for includegraphics
\usepackage{subfigure} % allows the use of subfigures
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} % used to insert code
\usepackage{listings}


%
%
%



%-----------------------------------------------------------------------
%  Modified fields
%-----------------------------------------------------------------------
\newcommand{\authorfirst}{Bradley}
\newcommand{\authorMI}{A.~}
\newcommand{\authorlast}{Schneider}
\newcommand{\degreefull}{Doc\-tor of Phil\-os\-o\-phy}  % force hyphenation at syllables if line breaks are required
\newcommand{\degreeshort}{Ph.D.}
\newcommand{\thesisordissertationlc}{Dissertation} % Make this uppercase
\newcommand{\dept}{Department of Computer Science and Engineering}
\newcommand{\institution}{Wright State University} % Doubting you will
                                % change this.
\newcommand{\thesistitle}{Some Really Great Title That Provides a
  Con\-cise View of the Topic Your Thesis Will Address} % Needes a line break
\newcommand{\bachdegreeshort}{B.S.} % Bachelor degree short
\newcommand{\bachinstitution}{Morehead State University} % Bachelor degree institution
\newcommand{\bachyear}{2012}% Bachelor degree year
\newcommand{\masterdegreeshort}{M.S.} % Bachelor degree short
\newcommand{\masterinstitution}{Wright State University} % Bachelor degree institution
\newcommand{\masteryear}{2017}% Bachelor degree year
%No spaces should be before or after this title.
\newcommand{\pdfsubject}{a short paraphrase of your title or focus of your thesis}
\newcommand{\pdfkeywords}{keyword 1, keyword 2, keyword 3, keyword 4}
\newcommand{\yearcomplete}{2020}
% set pdf file info
\usepackage{hyperxmp} % used to set pdf property info with \hypersetup command

%-----------------------------------------------------------------------
%  Thesis Advisor, Department Chair, Dean of Graduate Studies
%  I don't know why titles as separated... except in the one case at
%  the end.
%-----------------------------------------------------------------------
\newcommand{\thesisdirector}{Advisor's Name}
\newcommand{\thesisdirectortitle}{Ph.D., P.E., Get all of these right!}
\newcommand{\phdProgrameOrDeptChair}{Frank W. Ciarallo} % CHANGE TO: Faculty M. Name FOR MASTER THESIS
\newcommand{\phdProgrameOrDeptChairEducation}{Ph.D.} % CHANGE TO: Ph.D., P.E. ETC
\newcommand{\phdProgrameOrDeptChairTitle}{Director, Ph.D. in Engineering Program} % CHANGE TO: Chair, Department of Mechanical and \\ Materials Engineering FOR MASTER THESIS in MME


% COMMENT THE FOLLOWING 3 LINES FOR MASTER THESIS. PLEASE NOTE THAT FOR MASTER THESIS YOU SHOULD USE "\masterSignaturePage" AND FOR PHD DISSERATION "\phdSignaturePage" IN THE "approval sheet" SECTION.
\newcommand{\graduateSchoolDean}{Barry Milligan}
\newcommand{\graduateSchoolDeanEducation}{Ph.D.}
\newcommand{\graduateSchoolDeanTitle}{Interim Dean of the Graduate School}

%-----------------------------------------------------------------------
%  Final Examination Committee: Comment out the ones you don't need.
%-----------------------------------------------------------------------
\newcommand{\fecone}{Advisor, formal name, titles (see business card!)}

\newcommand{\fectwo}{Committee member 2}

\newcommand{\fecthree}{Committee member 3}

\newcommand{\fecfour}{Committee member 4}

\newcommand{\fecfive}{Committee member 5}

\newcommand{\fecsix}{Committee member 6}

\newcommand{\fecseven}{Committee member 7}

% If you have more committee members... good luck

% Modify this if needed for getting citations to "look right" according to your field. Read the natbib documentation on how to use this.
%\usepackage[round]{natbib}
%\usepackage{doublespace}

%=============================
%  Begin document!
%=============================
%
% Don't touch

% still don't touch.

% title sheet
\usepackage{WSUThesisTemplate/WSU}
\hypersetup{
                     pdfauthor={\authorfull},
                     pdftitle={\thesistitle},
                     pdfsubject={\pdfsubject},
                     pdfkeywords={\pdfkeywords},
                     }
% \normalem
\pagenumbering{roman}
\pagestyle{plain}
\rhead{\today}
\begin{document}
\maketitle
\doublespace\

% Still don't touch!!

%=============================
%  approval sheet
%=============================

\thispagestyle{empty}
\renewcommand\baselinestretch{2}
\begin{singlespace}
%\masterSignaturePage\newpage     % USE FOR MASTER THESIS
% You don't need both signature pages. Please comment out unnecessary lines in the .tex file. \newpage
\phdSignaturePage\newpage        % USE FOR PHD DISSERTATION
\end{singlespace}
%
%=============================
%  Abstract
%=============================
\newpage
\setcounter{page}{3}
\vspace{2in}
%
\begin{singlespace}
\begin{center}
  ABSTRACT
\end{center}
%
\noindent{\small{\authorlast, \authorfirst}.
		 {\degreeshort, \dept, \institution},
		 {\yearcomplete}.
		 {\sl \thesistitle}.}
\end{singlespace}
\vspace*{.5in}

\pdfbookmark[0]{Abstract}{Abstract}
%\phantomsection
%
%========================
% Start editing below.
%========================
The abstract should succinctly summarize summarize an the contents of the thesis, stating the problem, the procedure or methods used, the results, and any conclusions.  Doctoral dissertation abstracts should not exceed 350 words.  Master's thesis abstracts should not exceed 150 words.
%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------
%
%=============================
%  Table of contents, etc.
%=============================
%\renewcommand\baselinestretch{1.5}
\begin{singlespace}
\tableofcontents
\listoffigures
\listoftables
\end{singlespace}
%
%=============================
%  Acknowledgments
%=============================
\newpage
\thispagestyle{plain}
\setlength{\parindent}{0em}
\begin{center}
{\huge Acknowledgment}
\end{center}

I would like to take this opportunity to extend my thanks to\ldots\  If you have multiple paragraphs, the first should not be indented to match the style of the rest of the thesis.

\setlength{\parindent}{2em}
Any additional paragraphs should be indented as such.  That ugly command before this, you don't need to keep doing that. Remember to thank your advisor and committee members. Probably your Mom, others as you wish. 
%
%=============================
%  Dedication
%=============================
\newpage
\thispagestyle{plain}
\vspace*{3in}
\begin{center}
Dedicated to\\
Somebody special (Wife, husband, girlfriend or boyfriend works well
here.)
\end{center}
%
%
%
%=============================
%  Begin Chapters
%=============================
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}
\setlength{\parindent}{2em}

%================================================
\chapter{Introduction}
%================================================

%-------------------------
\section{Need for In-Home Monitoring}
%-------------------------
The number of people aged 65 and older in the United States is expected to reach 89 million by 2050, double the number of United States citizens in the same age group in 2011 \cite{Jacobsen2011AmericasPopulation}. As the elderly population continues to grow, so will the need to provide in-home care for the elderly. Elder care has traditionally been lacking qualified, certified practitioners and clinicians, and the demand for in-home health aides is very likely to exceed recruitment rates over this period of growth \cite{Rowe2016PreparingPopulation}. This will result in a growing deficit of qualified healthcare workers for in-home care, which will require the development of new models of healthcare delivery.

One response to this deficit of personnel is to leverage eHealth technology to provide in-home healthcare. In a 2005 report, the World Health Organization (WHO) defined eHealth as the “cost-effective and secure use of information and communications technologies in support of health and health-related fields, including health-care services, health surveillance, health literature, and health education, knowledge and research” and resolved to encourage long-term strategic plans to develop e-Health resources, noting that advances in technology have raised expectations for healthcare \cite{58thWorldHealthAssembly2005WHA58.28EHealth}. Since then, WHO has also formalized the idea of mobile-health, or mHealth, which is similar to eHealth but uses mobile technology. This includes in-home installation of sensors or imaging devices used to monitor patients with chronic illnesses \cite{WorldHealthOrganization2011MHealth:EHealth}. A recent survey of mHealth technology found that mHealth has been found as an effective way to monitor elderly patients suffering from dementia and cognitive disorders including screening for cognitive decline and promoting healthy habits in terms of physical activity \cite{Vazquez2018E-HealthReview}.

The need for in-home monitoring extends beyond caring for the elderly to monitoring individuals of any age who require assistance living independently due to a mental or physical disability or injury. In addition to the use of in-home monitoring for clinical purposes, it may be an important aspect of ubiquitous smart home systems. As more and more electronic devices are becoming internet-of-things (IoT)-enabled, including televisions, kitchen appliances, and security systems, the demand is growing for systems that automatically understand and react to users and their actions.


%-------------------------
\section{Activities of Daily Living}
%-------------------------
Activities of Daily Living (ADL’s) are defined as activities routinely performed in daily life, often for basic hygiene and personal care as well as food preparation, housekeeping, and other aspects of independent living. These activities have been studied since at least the 1960s \cite{LawtonAssessmentLiving} as an assessment for the ability to perform physical self-maintenance, especially among the elderly. Several scales of assessment have been proposed, each with the common purpose of evaluating the cognitive and physical capabilities of the subject \cite{LawtonAssessmentLiving, Bucks1996AssessmentScale, Carswell1993ActivitiesDisease}. In the clinical setting, it has been shown that the ability to independently perform ADLs is an indicator of the onset and progression of conditions such as Dementia and Alzheimer's disease \cite{Desai2004ActivitiesTreatment}.

The manual assessment of human activities is dependent on the ability for the subject to be observed, necessitating the use of either in-home visits by a clinician or visits by the subject to clinical laboratories. This can be challenging when considering the expense of home visits and the limited ability of elderly to travel and coordinate appointments. The use of technology through eHealth and mHealth applications to monitor and observe ADLs in a home environment can greatly reduce the impact to both the subject and clinician by providing an unobtrusive, automated method of gathering activity data. [what to cite?]

A challenge to implementing automated activity detection and recognition systems is the need for a formalized model defining each activity under observation. The ability to recognize activities as they occur comes very naturally for humans, but it is much more difficult to define for a machine. Human activities are defined in overlapping temporal and semantic spaces. That is, one higher-level activity may include the performance of several lower-level activities at the same time. Thus the definition of these activities may be subjective to the observer. 

For example, a subject may perform the high-level activity of preparing breakfast, which involves many actions and movements in a sequence over a significant period of time. Within the action of preparing breakfast, the subject may have accomplished the lower-level activity of making toast, and within that activity the subject may have accomplished the lower-level activity of inserting bread into the toaster. The chain of activities may continue decomposing all the way down to the activity of picking up a bag of bread. Understanding the same or overlapping activities at all of these levels is crucial to a fully functioning activity monitoring system.


%-------------------------
\section{Research Goals}
%-------------------------
\emph{Research Question 1: Can wearable sensors be used to unobtrusively monitor activities of subjects in a semi-structured environment?}
A desired outcome of this work is to provide evidence that a system based on wearable sensors may be used in the home (i.e. not in a clinical setting or laboratory) to provide continuous monitoring of activities of daily living. The outputs of the system provide data on activities and patterns of activities to inform clinicians on possible changes in the health and wellness of the user. Data collected at clinical laboratories is temporally sparse and involves scripted movements, bringing into question the validity of the data for assessing the subject’s ability to perform the actions in a natural environment; by contrast, the data collected by in-home monitoring will show a more complete look into the subjects’ abilities to perform activities on a day-to-day basis.
Early work was focused on detecting and measuring gait in controlled and semi-controlled environments \cite{Schneider2017PreliminaryProcessing, Schneider2019ComparisonEnvironments}. While results were achieved with uninterrupted gait sequences of several meters, it was clear that long uninterrupted gait sequences are uncommon in many daily activities, which include starts and stops and navigation around household objects. For this reason, the ultimate focus of this work is on object interactions rather than gait, as object interactions provide a much richer context for activities that are being performed.

\emph{Research Question 2: Can features extracted from first-person videos recorded in subjects’ homes be used to categorize different interactions with objects?}
The use of object interactions to determine activities has been popular in recent automated activity recognition and classification systems \cite{Pirsiavash2012, Sudhakaran2018, Nakatani2018PreliminaryKnowledge, Wang2018, Gokce2019HumanPairs}. Many in-home ADL’s are at least partially defined by interactions with certain objects. For example, knowing that a subject is interacting with a remote control should reveal that the subject is interacting with a television or other electronic device. The search space of candidate activities can be greatly culled by knowing which objects are involved. A desired outcome of this work is to exploit a combination of video and object features to characterize different interactions with objects.
Building on an initial set of features from previous work with gait, an initial baseline set of features was identified for categorizing object interactions. In addition to the location of objects, this set of features was heavily dependent on optical flow features to identify changes in object location and color features to identify changes in the appearance of objects [cite FUZZ-IEEE]. Additional experiments are needed to extend and refine this set of features and the resulting Fuzzy Inference System (FIS) that was developed, but the initial work proved the feasibility of using first-person video features to categorize object interactions.

\emph{Research Question 3: Can the activity identification system a) accurately describe activities and b) handle uncertainty in the results better than current state-of-the-art methods?}
While much recent work has been done on using deep learning and neural networks to identify human activities in video sequences (such as run, walk, sit down, throw, turn on faucet, open refrigerator, etc.) \cite{Abebe2016, Ozkan2017, Li2016}, there are two major drawbacks to the current deep learning trend:
\begin{enumerate}
    \item The networks must be trained to recognize a very specific set of actions. To date, most work involving neural networks has been against small sets of precisely defined and scripted activities, limiting practical applications.
    \item Since deep learning requires no features to be identified/extracted, the use of these technologies in isolation provides no ability to explain the outcome – only to identify it. That is, when the network fails to provide the correct answer, there is no explainability of the result or the misidentified activity.
\end{enumerate}

We propose the use of fuzzy logic to address these limitations. Many activities are very similar in their basic movements and absolute identification may become difficult as the set of activities grows. If an unknown activity occurs, fuzzy outputs may still provide some insight into the activity or similar activities. This is an advantage over machine learning approaches which will simply fail to identify the behavior. The simplicity of a fuzzy system can be seen in different lights; our goal is to show that the simpler fuzzy model is capable of providing a rich description of activities. In the course of attempting to identify an activity, the fuzzy system will produce interpretable fuzzy variables (as opposed to unexplainable deep learning features) which describe actions in the scene  - for example, a specific object in the scene is being interacted with and the subject is manipulating the shape of the object. While the overall activity might be unidentified, a good amount of information may still be taken away from the fuzzy system.


%-------------------------
\section{Thesis Statement}
%-------------------------
This dissertation proposes to bridge the gap between in-home activity monitoring leveraging an object recognition framework to using a fuzzy linguistic framework to handle uncertainty in complex movements between humans and objects in an in-home unstructured setting.


%================================================
\chapter{Related Work}
%================================================
Related work varies based on several factors - the types of activities detected, the sensors used to detect the activities, and the computational intelligence methods used to convert sensor data into activity information.

\section{Activities Being Detected by Vision Sensors}

A number of activities have been found in related work on detecting and classifying activities using vision sensors. These related works were grouped into three categories:
\begin{enumerate}
\item Anomaly Detection, 
\item Full-body Activities, and
\item Partial-body Activities.
\end{enumerate}

\subsection{Anomaly Detection}

Of the 26 studies that were included in our results, five described systems designed to detect activities that were considered to be anomalies. Each of these systems used full-body views of subjects from vision sensors combined with computational intelligence to observe an environment and determine when either an unexpected activity occurred, or when an activity occurred in an unexpected way. 

In \cite{Beleznai2012}, a vision sensor monitored a pedestrian area and used motion history images to identify activities that were determined to be anomalies, such as vehicles entering the space, by means of clustering. This method approximated the spatio-temporal distribution of activities that occurred frequently and detected activities that did not fit that distribution. Another outdoor environment - a loading dock - was monitored for anomalous activities in \cite{Hamid2005}. Activities were represented as n-grams of events and a distance metric based on these n-grams determined the dissimilarity between activities. Anomalous activities that were discovered included a truck leaving the dock with its door open and an unusual number of people unloading a truck. 

Anomalous activity detection was also found in indoor environments. Fuzzy membership functions were used to captured several parameters of indoor human activities (location, time, perceived area of subject) from an omnidirectional vision sensor and output a fuzzy determination of how normal the activity was \cite{Seki2009}. \cite{Yiping2006} presented a similar experimental setup with an indoor omnidirectional vision sensor seeking to detect anomalous activities, but used Gaussian models of time and space to determine whether an activity was normal or not.

\subsection{Full-body Activities}

While detecting anomalies is useful for some applications, detecting the occurrence of a specific predefined activity is important for other applications. In 10 of the 26 reviewed studies, the vision sensor recorded subjects’ full bodies to detect specific activities with computational intelligence. 

One of these 10 studies described a system designed to detect falls. This work is particularly applicable in living facilities for the elderly, where detection of falls is critical. \cite{Banerjee2014} compared the use of three different vision sensors in an elder-care facility to detect when a subject fell. The system was capable of detecting three fall-related activities - being upright, sitting, and being on the floor.

Another two of the 10 studies were focused on using full body video sequences to detect activities in a medical setting. These environments, such as a trauma unit or patient room, may be very busy with doctors and nurses, and activities may be happening frequently or simultaneously. Because of this, both of the reviewed studies required systems capable of monitoring several subjects at the same time to identify activities \cite{Chakraborty2013, Bloisi2009}.

The remaining seven of 10 studies which used full-body views from vision sensors were focused on detecting indoor activities of daily living. While the specific sets of activities varied somewhat from study to study, each of these were deployed in home-like environments and detected daily activities such as sitting on a sofa \cite{Figueroa-Angulo2013}, eating dinner \cite{Yao2016}, or reading a book \cite{ElHelw2009}. 

\subsection{Partial-body Activities}

Ten of the studies included in our systematic review detected activities using vision sensors focused on only part of the subjects’ bodies. In these cases, a focused view of specific parts of the body, rather than the entire body, was determined to be beneficial.

A fall detection system based on a wearable camera was described in \cite{Mahabalagiri2013}. However, this work used only a partial view of the subject’s body, since the camera was worn on the torso. This system was capable of detecting a set of three activities similar to the full-body fall detection work \cite{Banerjee2014}, including sitting down, lying down, and falling.

Three of the 10 studies applied partial-body views from a vision sensor to detect speaking activities. These studies were concerned with identifying speakers in multi-user or noisy environments, citing use-cases such as human-robot interaction \cite{Yoshida2010, Lim2009} and video calling \cite{Savran2018}. Though the recognition of the audio is important in each of these cases, the vision sensor provides valuable information for detecting the beginning and continuation of a speech activity, based on the movement of the subjects’ mouths.

Two of the 10 studies which used only partial-body information were focused on detecting activities while driving. These studies had a similar goal of detecting drivers’ actions while the car was in motion. One study sought to build an understanding of actions that occur as the driver completes maneuvers such as turning or approaching an intersection \cite{Martin2017}. The other study was concerned with detecting both driving activities (steering, operating the shift level) and non-driving activities (eating, using cell phone) \cite{Zhao2012}.

Another two of the 10 reviewed papers described systems which used partial-body views from a vision sensor to detect activities of daily living. In both of these cases, the vision sensor was attached to the subject, either a human or a robot, and computational intelligence was applied to make a determination of the current activity being performed \cite{Li2016, Wu2007}. Due to the positioning of the vision sensors, these studies have only a partial-body view of the subject. Unlike the studies detecting daily activities from full-body views, the partial-body view does not provide the visual information to determine the subject’s complete posture. These studies choose instead to exploit the information of objects or other people within view to detect the occurrence of an activity.

The final study that detected partial body activities focused on detecting interactions between a subject and elements of a museum exhibit \cite{Baraldi2015}. The described system allowed the user to interact with the exhibit by wearing a vision sensor and making predetermined hand gestures in various locations.

\subsection{Discussion}
Our results reveal a considerable degree of diversity in the types of activities to which activity detection via vision sensors and computational intelligence has been applied. Overall, the most common type of detected activities were indoor activities of daily living \cite{McIlwraith2009, Wu2007, Yao2016, McIlwraith2008, ElHelw2009, Rowe2007, Figueroa-Angulo2013, Mahabalagiri2013, Ong2013, Li2016}. However, we also found results focused on detecting activities in medical settings \cite{Chakraborty2013, Bloisi2009}, in vehicles \cite{Zhao2012, Martin2017}, in the workplace \cite{Hamid2005}, and interactions with robots \cite{Yoshida2010}. A conclusion may be drawn that the vision and computational tools discussed have broad utility to intelligent systems which must understand the actions of users, regardless of the particular domain, setting, or application of that knowledge.

A drawback to the diversity of the activity domains in our results is that it remains difficult to draw direct comparisons between methods which have seen success. A vision-based system which performs well inside of a home may not perform well when deployed to detect activities in an outdoor work environment. Even within the same activity domain, we did not see a standard dataset emerging to provide a common baseline. In most of the studies, a new dataset was created, and little information was provided on how these datasets were captured. Further, datasets are typically gathered specifically for the evaluation of the system rather than from a truly operational deployment of the system.


%================================================
\section{Types of Vision Sensors}
%================================================
Our analysis revealed that several types of vision sensors have been used for activity detection. These sensors generally fell into the following four categories: 
\begin{enumerate}
    \item traditional fixed RGB vision sensors,
    \item fixed RGB-depth vision sensors,
    \item wearable RGB vision sensors, and
    \item wearable RGB-D vision sensors
\end{enumerate}
    
In some cases, several different devices were used either in concert or separately to provide the best results.

\subsection{Fixed RGB Vision Sensors}

Traditional RGB cameras come in a variety of styles, configurations, and sizes, and have been a frequent choice for vision-based activity recognition applications. Of the 26 studies that met our inclusion criteria, 16 used one \cite{Yoshida2010, Savran2018, Beleznai2012, Wu2007, Seki2009, Yiping2006, McIlwraith2008, ElHelw2009, Rowe2007, Lim2009, Chakraborty2013, Zhao2012} or more \cite{McIlwraith2009, Bloisi2009, Martin2017, Hamid2005} fixed RGB vision sensors. In each of these studies, the vision sensors were statically deployed in the environment within which the recognized activity is taking place, providing a third-person view of the subject(s). Within the category of stationary RGB cameras we see variation amongst the types and number of devices used, depending on the specific challenge being addressed.

One challenge that researchers face when using fixed (stationary) RGB vision sensors is a limited view of the environment, either due to a limited angle of view or due to occlusion by other objects. However, some researchers have been able to alleviate this issue. For example, three studies within our sample used multiple cameras simultaneously to provide a more complete view of the monitored environment. In \cite{McIlwraith2009}, a camera network was used to provide a comprehensive view of several seating stations within one room. The system mimics a home environment where activities such as dining, studying, and reading may be expected to occur in distinct locations. Individual camera nodes may provide sub-decisions, but communication between all nodes leads to a final result. The other studies that used multiple cameras examined differing use cases. For example, in \cite{Hamid2005} multiple cameras were deployed with partially overlapping viewpoints in order to provide complete coverage of activities that transpired in a loading dock. This allowed detection of multiple activities occurring in parallel at a given time.  Multiple cameras were also used in \cite{Martin2017} to provide detailed images of the face and hands of a subject driving a vehicle to analyze the coordination of the head and hands while maneuvering the vehicle. The cameras were positioned to optimally capture the relevant areas of the subject while excluding irrelevant details such as the dashboard or the passenger section of the car.

Our results also showed that researchers have found it possible to achieve an enhanced perspective without the use of multiple cameras. Two of the studies in our sample incorporated omni-directional vision systems into the design of their activity recognition systems \cite{Seki2009, Yiping2006}. These cameras map a complete 360 degree view onto a typical two-dimensional intensity image, providing full monitoring of an area with a single hardware device, typically mounted overhead in the center of the space. Though this results in a distorted image due to the 360-degree perspective, the image may still be processed and human postures are still reliably discernable \cite{Seki2009}. A similar single-camera experimental setup is shown in \cite{Yiping2006} to effectively monitor an entire panoramic view of a room. The omnidirectional vision device is capable of monitoring activities occurring in a seating area and four areas of entry or exit to other rooms.

Another challenge in detecting activities with fixed RGB vision sensors is the lack of depth information from the scene. That is, only two dimensions of data are recorded, leaving out potentially valuable information about the movement and positioning of objects. However, traditional RGB cameras (which do not directly record depth information) can provide depth information when used in pairs as a stereo camera. Depth may be inferred by comparing the perspective of each camera given the predetermined distance between them. Two of the studies included in our systematic review take this approach \cite{Lim2009, Bloisi2009}. In \cite{Bloisi2009}, depth information from a stereo camera is exploited to detect the background in each frame and subsequently remove it, leaving only foreground objects for consideration in activity recognition. Depth information from stereo cameras has also been used to determine distances between multiple subjects in the same area \cite{Lim2009}. The precision afforded by the depth information allows for improved facial tracking and differentiation between subjects in closer proximity to one another, improving the overall ability to differentiate the active speaker.

\subsection{Fixed RGB-Depth Vision Sensors}

As previously discussed, traditional RGB vision sensors record only a two-dimensional mapping of the three-dimensional environment. This is an important limitation because it is not possible to reconstruct the third dimension without the use of a standard vision system that integrates multiple vision sensors to provide different perspectives of the same area (i.e. a stereoscopic camera). 

An alternative approach to providing depth information from vision sensors is to use an RGB-Depth sensor. These sensors typically incorporate a traditional RGB camera, an infrared (IR) camera, and an IR projector. The IR projector projects a pattern of IR light onto the scene which is recorded by the IR camera to determine depth. The depth measures are combined as an extra channel of information with the image captured by the RGB camera. Six of the studies within our sample used RGB-D vision sensors \cite{Banerjee2014, Yao2016, Akbari2017, Zhao2017, Figueroa-Angulo2013, Ong2013}.

The methods described in \cite{Ong2013, Yao2016} detect activities based on sets of joint features describing the flexion, extension, rotation, or position of a variety of human joints. The additional depth information from the vision sensor is required in this case to provide the necessary inputs to the recognition algorithms. \cite{Figueroa-Angulo2013} describes a similar approach where digitized three-dimensional skeletal and joint models are extracted from the depth images captured by the sensor and then used to detect activities based on a statistical Markov model. A multi-modal sensing system built a human motion model in \cite{Akbari2017}. An RGB-D vision sensor is paired with wearable measurement devices (IMU, gyroscope, accelerometer) to observe motions, and determine situations in which each device is better-suited to capture the data.

Another convenience provided by the use of an RGB-D vision sensor is improved performance in poor lighting conditions. While RGB vision sensors may have difficulty properly adjusting exposure and maintaining a clear image in bright or low light, the IR projectors and cameras in RGB-D vision sensors provide a consistent result in most lighting conditions since they are not susceptible to changes in visible light and have usually have an integrated source of IR light to illuminate the scene. \cite{Banerjee2014} demonstrates this and gives a comparison of human silhouette extractions using a webcam with an IR filter and an RGB-D vision sensor in an eldercare facility. The RGB-D sensor is shown to be effective, though the depth information begins to degrade as the subject moves farther from the vision sensor (due to natural scattering of IR light from the projector over this distance).

We also found that depth information may benefit systems that need to distinguish between multiple subjects. An RGB-D vision sensor was used in \cite{Zhao2017} to provide biometric identification of users based on bone lengths. The authors acknowledged that while previous work indicates that fingerprints or iris scans are more reliable means of identification, the model built by an RGB-D sensor is much more user-friendly and allows passive user identification from a distance, which may be important in a dynamic multi-user environment such as a workplace. 

\subsection{Wearable RGB Vision Sensors}

A drawback to all of the static vision sensors discussed to this point is that they are not portable. They only record data within a fixed environment, which may not be sufficient for all use cases. For example in an indoor environment, if the person moved away from the living room where the sensor was placed, the sensor would fail to raise an alarm if the person fell down in the bedroom or the bathroom. Four studies in our sample attempted to overcome this limitation by using wearable vision sensors \cite{Radhakrishnan2016, Mahabalagiri2013, Li2016, Baraldi2015}. The sensors were placed on the subject to provide a first-person view of the activity being performed. In many cases, this meant that features describing the posture and joints of the subject were no longer available because they were out of view, but the vision sensors may have been better positioned to record information about objects that the wearer interacted with, or people with which the wearer interacted. Of the four studies that used wearable vision sensors, three used RGB vision sensors \cite{Radhakrishnan2016, Mahabalagiri2013, Baraldi2015}.

When attempting to recognize activities from images recorded with wearable vision sensors, it is common to use detailed clues from the scene in lieu of posture information (which cannot be captured because the body is not in the field of view of the sensor). In \cite{Radhakrishnan2016}, a system was described which attempts to identify the action of picking an object off of a shelf in a store. The action was detected via non-vision sensors, but the identification of the specific object proceeded via images captured by a wearable camera. The camera was worn on the wrist, such that it may record any chosen object as the interaction occurred.

In other cases, the wearable vision sensor can provide information about specific hand gestures instead of objects with which the subject is interacting. This is demonstrated in \cite{Baraldi2015}, where a vision sensor worn as a pair of glasses provided a view of the subject’s hands. Hand gestures were recognized and applied to the object in the subject’s gaze. The glasses and hand gestures presented a natural means of interacting with the system which would not be achievable with a stationary vision sensor.

A final use case we found for wearable vision sensors was to describe the motion of the subject wearing the device. When mounted in a fixed position on the body, movement recorded by the sensor across temporal image sequences may be attributed to motion in that part of the body, supplying information about both the environment in front of the subject as well as the posture of the subject without the subject being directly in the field of view. In one study, an RGB sensor was worn on the subject’s trunk and the movement of the camera was used to estimate the movement of the torso, enabling the detection of sitting and lying down without any part of the torso in view \cite{Mahabalagiri2013}. This is a good example of how wearable sensors may be used to capture derivative information from the scene, in addition to information of objects or subjects that are directly within the view of the sensor.

\subsection{Wearable RGB-D Vision Sensors}

Only one study employed wearable RGB-D vision sensors for the purpose of activity recognition. The approach in this study was based on object interaction, and a robot (rather than a human) was equipped with the wearable RGB-D vision sensor \cite{Li2016}. The purpose of the study was for the robot subject to become aware of the activity it was performing by using knowledge of nearby objects. The wearable sensor was used to detect the objects interacting with the robot’s hands, and that data was fused with joint features provided from the robot’s operating system, taking advantage of the availability of both object and posture data. 

\subsection{Secondary Sensors}

In nine of the 26 reviewed studies, secondary non-vision sensing devices were used to enhance the data captured by the vision sensor \cite{Yoshida2010, Radhakrishnan2016, Savran2018, Wu2007, Akbari2017, McIlwraith2008, Zhao2017, Lim2009, Li2016}. Three of the included studies used a microphone to assist in voice activity detection along with the vision sensors \cite{Yoshida2010,Savran2018,Lim2009}. The vision sensor was important to these studies for identifying the speaker in the observed space, and the microphones were used to provide auditory data. The only study which used a wearable RGB-D device mounted the device on a robot, and used data from mechanical devices to supplement the visual data with angles and positions of joints of the robot \cite{Li2016}. This provided a full set of posture data, which is typically lacking in situations in which the vision sensor is mounted on the subject. Two of the reviewed studies made use of smart watch devices to provide details of wrist motion to computational intelligence methods \cite{Radhakrishnan2016, Zhao2017}. In \cite{Wu2007}, radio frequency identification (RFID) tags were applied to objects and the RFID device was used to correlate vision sequences with proximity to objects in the environment. \cite{McIlwraith2008} combined an ear-worn accelerometer with a vision sensor and extracted non-overlapping discriminatory features from each device to detect activities. Similarly, \cite{Akbari2017} paired a vision sensor with an IMU device. The IMU device provided finer details of the motion the subject performed, allowing the activity detection to be fine-tuned based on the speed of the movement. These studies demonstrate situations in which multiple sensing devices and/or different sensing modalities may combine to provide a richer dataset for activity detection.

\subsection{Discussion}

In our results, we found a variety of vision sensors, including both fixed and wearable RGB and RGB-D sensors. Additionally, we found that there were significant variations on each of these, such as omni-directional or stereoscopic sensors. Our results indicate that each of these are capable of providing positive results in activity detection when used with computational intelligence. 

We note that the choice of vision sensor is largely dependent on two factors - 1) the scene in which activities will take place and 2) the types of activities that need to be detected. Wearable sensors are frequently chosen when the scene is not a fixed location (i.e. the activity may occur in any location in which the user is present) \cite{Baraldi2015} or when objects within the user’s perspective are more important than the overall posture or positioning of his limbs and joints \cite{Baraldi2015, Li2016, Radhakrishnan2016}. When choosing between RGB and RGB-D sensors, RGB-D sensors are a common choice when differentiation of activities necessitates three-dimensional data \cite{Ong2013, Figueroa-Angulo2013} or when lighting conditions may affect the performance of visible-light-based RGB sensors \cite{Banerjee2014}.

Overall, it is clear that there is no single best vision sensor for activity detection. The environment and the type of activity being detected may require consideration. Ultimately, several types of sensors are capable of providing data to successfully detect activities.

%================================================
\section{Computational Intelligence Methods}
%================================================

Our analysis indicated that a wide variety of computational intelligence methods have been used with vision sensors. We categorized studies by algorithm and found the use of both supervised and unsupervised methods. In general, supervised methods are a frequent choice when the desired output from the system is an explicit activity label from a set of predefined activities. Applications which need only to detect the abnormality of an activity often turn to unsupervised intelligence methods since these methods do not require the manual annotation of truth data, which can be time consuming. We review both methods in relation to the use of vision sensors and activity detection below. 

\subsection{Stochastic Modeling}

Four of the reviewed studies employed some variant of a stochastic process model \cite{Rowe2007, Figueroa-Angulo2013, Lim2009, Chakraborty2013}. The activity detection capability of a multiple vision sensor network described in \cite{Rowe2007} was demonstrated by a Markov model showing regions of activity across multiple rooms with transition probabilities determined by a collected dataset. By contrast, a large composite Hidden Markov Model (HMM) comprising smaller HMMs was built in \cite{Figueroa-Angulo2013}. The smaller HMMs individually specified the states within just a single activity and contained transitions back to a common initial and final state. The larger HMM was composed by allowing transitions between the common initial state, which represented transition between the smaller individual activities. The HMMs were trained on features representing the skeletal structure of the observed subject. In a different application of the Markov property, a Markov logic network (MLN) is employed in \cite{Chakraborty2013}. The MLN is driven by an activity grammar that describes steps within an activity and forms relationships between predicates and objects (e.g. “stethoscope approaches patient”). A Langevin process model, rather than a Markov model, was used in \cite{Lim2009}. This work fused visual and audio data to determine speaking activities and identify speakers in a multi-user environment. 

\subsection{Fuzzy Logic and Clustering}

Activity detection methods based on fuzzy logic were used in four of the reviewed studies \cite{Banerjee2014, Seki2009, Yao2016, Akbari2017}. Fuzzy methods introduce degrees of truth, rather than binary decisions. Fuzzy C-Means clustering was used in \cite{Yao2016} to turn training posture feature vectors from an RGB-D vision sensor into fuzzy rules to support classification via type 2 fuzzy logic. An extension of Fuzzy C-Means clustering, Gustafson-Kessel clustering, was used to cluster silhouette and image moment features to recognize activities using RGB-D sensors \cite{Banerjee2014}.

In addition to fuzzy clustering, we also found other applications of fuzzy logic. Inputs from an omni-directional vision sensor were recorded in terms of fuzzy variables in \cite{Seki2009}. The use of fuzzy variables improved the performance of the activity detection around the discretizing borders when compared with previous work using Bayesian methods \cite{HirokazuSeki2008}. 

\subsection{Bayesian Methods}

Our results also included three studies that used Bayesian methods to detect activities \cite{McIlwraith2009, Yoshida2010, Wu2007}. \cite{McIlwraith2009}, which employed a network of several vision sensor nodes, proposed a greedy structure learning algorithm based on the Bayesian Information Criteria (BIC). Each node built a Bayesian network to learn activities for which the node was a candidate. Candidate nodes contributed data to the network to make a final determination of an activity. The distributed approach allowed sub-decisions to be made at the node level, which positively impacted the fault tolerance of the network. 

A Dynamic Bayesian Network (DBN) was used in \cite{Wu2007} to automatically acquire models of activities and objects that are involved with them. The DBN was formed from internet-based knowledge repositories, object information, visual frames, and RFID inputs. Learning was done in an unsupervised manner so that data did not need to be labeled (except for the purpose of testing).

A third study used a Bayesian network to perform voice activity detection from a combination of RGB vision sensor data and audio \cite{Yoshida2010}. The inputs included the log-likelihood of silence in audio data calculated by a speech decoder, a feature based on the height and width of the lips as captured by the vision sensor, and the confidence that a face was detected in the visual data. Probabilities for the Bayesian network were obtained from a Gaussian Mixture Model (GMM) trained in advance on a separate set of training data.

\subsection{Gaussian Mixture Models (GMM)}

Gaussian modeling, another probabilistic method, was used in three of the examined studies \cite{McIlwraith2008, ElHelw2009, Yiping2006}. In \cite{McIlwraith2008, ElHelw2009}, GMMs were used to perform fusion of data from multiple sensors for the detection of activities. The models were based on data provided by a vision sensor and an ear-worn accelerometer. Vision sensors provided features such as bounding box aspect ratio and eigenvectors of silhouettes, and average optical flow within the blob, while the wearable device provided information on the tilt and movement of the head. \cite{Yiping2006} used Gaussian distributions to build spatial and temporal models of activities. Using the combination of these models, activities were determined to be either normal or abnormal. Recorded activity information was then used to update the models. 

\subsection{Neural Networks}

Deep learning and neural networks were used in two of the reviewed studies \cite{Li2016, Savran2018}. Multiple types of Recurrent Neural Network (RNN) structures were used in \cite{Li2016}. These structures were shown to be well-suited to performing classification based on sequential video frames. Convolutional layers of the networks operated on pre-processed visual frames which were later combined with joint features as input to a Long Short-Term Memory (LSTM) layer. The LSTM layer determined when a tracked hidden state should be updated, contributing to the strong performance of the system against the temporal video data. Similar positive results were shown against temporal data in \cite{Savran2018}, where audio data was input to deep learning detectors after visual sensor data was used to detect speaking activities. Speaking activity was visually detected by analyzing motion near the mouth.

\subsection{Other Methods}

The previously discussed methods account for a majority of the computational intelligence methods found in the examined studies. However, a variety of other methods were also used to detect activities using vision sensors. A simple decision tree was used in \cite{Radhakrishnan2016} to detect the single activity of picking an item from a shelf. In another study, the K-means clustering algorithm was used to cluster unlabeled skeletal features into differentiable activities \cite{Ong2013}.

A more complex density-based clustering algorithm was used in \cite{Beleznai2012} to detect rare activities using binary space-time descriptors. Two of our included studies used a Random Forest classifier to detect driving activities \cite{Zhao2012, Martin2017}. These were the only studies in our results to use this method. 


\subsection{Discussion}
We also found that numerous computational intelligence methods have been used with vision sensors to detect activities. As with the choice of vision sensor, this may be driven by the set of activities that need to be detected. In applications requiring only a measure of normality for an activity (e.g. normal vs. abnormal) rather than a specific classification of the activity in a given set, clustering methods are shown to be robust \cite{Beleznai2012}. In applications where activities of daily living are detected, we see several distinct methods including Bayesian methods \cite{Wu2007, McIlwraith2009}, Stochastic modeling \cite{Rowe2007, Figueroa-Angulo2013}, and Gaussian Mixture Models \cite{ElHelw2009, McIlwraith2008}. As mentioned earlier, the differences in the experimental setup of these similar studies present a difficult comparison of methods against each other. 

A commonly cited factor in the success of computational intelligence methods in classifying an activity into a predefined set of possible activities is the ability for the method to take into account the various states within an activity. Activities typically have both spatial and temporal components, and our results indicate strong performance with methods that can either consider time-aggregated data or have a memory of previous outcomes. This typically manifests as the ability for the algorithm to track a state variable. Examples of this are seen in neural network and deep learning methods \cite{Savran2018, Li2016}, stochastic modeling methods \cite{Rowe2007, Figueroa-Angulo2013, Chakraborty2013, Lim2009}, and Bayesian methods \cite{Wu2007}.

Comparing computational intelligence methods and the results associated with each was complicated by the lack of consistency in reporting results. Several of the reviewed studies failed to report an accuracy metric or the details of the dataset they collected. We encourage authors to report these salient details in their future studies.

%================================================
\chapter{Methods}
%================================================

%-------------------------
\section{Image Features}
%-------------------------

%-------------------------
\section{Statistical Methods}
%-------------------------

%-------------------------
\section{Fuzzy Inference Systems}
%-------------------------

%================================================
\chapter{Experiments}
%================================================
%================================================
\chapter{Results}
%================================================



%%=============================
%\chapter{In The Beginning}
%%=============================
%The date of this document generation (current version of this document) is the date of the thesis on page two (\today).{}
%sdafsd sadfsdaf asdfadf. OK, I have nothing to say here, but you should in your thesis/dissertation. Introduce your chapter in a page or so.
%
%On a side note, style files should be located in the texmf tree automatically when a package is installed.  The sty-files used for this template are located in the folder \lq\lq{}sty\_files\_if\_needed\rq\rq{}.  If you do not have the packages already installed, you can also simply move or copy them into the same directory as this file.
%
%%=============================
%\section[Citations/References (short form of section name)]{Citations, using them, referring to them, formatting them, and loving using them because if you don't use them when appropriate it's called \emph{plagiarism}, and references}
%%=============================
%That was a ridiculously long section name to illustrate how to get a shorter version in your table of contents.
%
%See Section~\ref{sec:equations}. I'm not the only one who says this is awesome stuff\cite{Mortara2004}!
%This citation is in the ASME format for which I've included a \verb'bst'
%file. See \textsc{asmems4} in the source document (way near the end). Some  other options are:
%\begin{itemize}
%\item natbib:  put \verb'\usepackage{natbib}' in the header (before \verb'\begin{document}'), and \verb'\bibliographystyle{plainnat}' (just before \verb'\bibliography').  See the natbib documentation for details.
%\item AIAA:\@put \verb'\usepackage{overcite}' in the header (before \verb'\begin{document}'), and \verb'\bibliographystyle{aiaa}' (just before \verb'\bibliography'). Use \verb'\citen{sdfsdf}' for in-line citations. Read the overcite package documentation for more.
%\end{itemize}
%
%A variety of other formats are available on \href{http://www.ctan.org}{CTAN} or though an internet search. You may also just pick one using ``natbib''.\footnote{Don't those quotes look bad before ``natbib''? Well, use two left quotes in \LaTeX\ to get it to look right.}  The formatting of your references is controlled by the \verb'bibliographystyle' command near the end of the document. Use the bibliography style appropriate to your field. On exists, out there. I'm sure. If not (ok, it happened to me 10 years ago), you can use the \verb'makebst' script to make your own. Answer a bunch of questions and it makes a style file for you. If you are using a numerical citation system, you may want to use \verb'\usepackage{cite}'. It creates a condensed numerical list of citations, but can cause conflicts with the \verb'hyperref' package (you may need to decide\ldots sorry, this is a bug that drives me nuts too.)
%
%%=============================
%\subsection{About the Bibliography}
%%=============================
%There are two lines in this section in your \LaTeX\ file. The first is a bibligraphystyle command (see the \textsf{tex} file. Don't move it elsewhere. It won't work out well for you.
%
%You need to choose a style file that formats references the way you want them formatted.  You should chose a style for your major.  Look for bst styles on ctan, or use makebst.sty. This formats the bibliography.
%The last line is the bibliography command. It just tells \LaTeX\  the name of your \textsf{.bib} file. This is a database of your references.
%
%Also, use the \verb'\phantomsection' command to correctly anchor the hyperlink for the bibliography.  Without this, any hyperlinks in the document, and the link from the bookmarks will take you to the incorrect page.
%
%You can have as many sources listed in the *.bib file, but if you do not cite them in your document, they will not show up in your Bibliography.  And if you do, they automatically are sorted\ldots pretty nice!
%
%%=============================
%\subsection{Equations}\label{sec:equations}
%%=============================
%\begin{equation}\label{eq:alphaoverbeta} %You can name the label anything you want, just not the same as something else
%x=\frac{\alpha}{\beta}
%\end{equation}
%Really bad idea: don't start a section with an equation. Don't start a sentence with a variable either. Start with a word. In the body of the text, use the \$ around the variable.  For example, the variable $x$ is the distance. How hard was that?
%
%Sometimes you don't want an equation  number, so use \verb'{equation*}' instead.
%\begin{equation*}
%x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
%\end{equation*}
%
%If you need to align a set of equations up use the \verb'align' command instead with the use of the \& to set the anchor
%\begin{align}
%x&=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\\
%x&=\frac{\alpha}{\beta}
%\end{align}
%The same applies for the \verb'align' command\ldots if you don\rq{}t want equation numbers,  just use an *.  More can be found at the references listed in Section~\ref{sec:refs}.  You can easily make matrices such as the viscous damping matrix, $C_d$, which is shown in Equation~\eqref{eq:damping_mat}.  Use the \verb'\eqref{eq:\ldots}' command to reference equations properly.
%%
%\begin{equation}
%\label{eq:damping_mat}
%C_{da} =[U_n^Tnn]^{-1}
%\left[ \begin{array}{ccc}
%a&b&c\\
%	\ddots & 				& \\
%	     	& 2 \zeta_i \omega_i 		& \\
%		&				& \ddots \\
%\end{array} \right]^{{-1}} [U_n]^{-1}
%\end{equation}
%
%We don't number equations in \LaTeX. \LaTeX\ does it for us. Label them with names (see the raw \LaTeX\ file).   Just don't put a space in the middle of a variable name.
%
%Now if I have an equation that I want to be between paragraphs, unlike equation (\ref{eq:alphaoverbeta}), I put a blank line after the equation.
%\begin{equation}
%  \label{eq:anothersillyequation}
%  x\neq y
%\end{equation}
%
%See  indent? But if I'm continuing the paragraph, don't put that blank line in
%\begin{equation}
%  \label{eq:anothersillyequation2}
%  x\neq y
%\end{equation}
%and there won\rq{}t be an indent.
%
%If you want a new page and you have a figure you want to stay with the section, you need to use the \verb'\clearpage' command instead of the \verb'\newpage' command.
%
%%=============================
%\section{Figures}
%%=============================
%This is not the same as Section~\ref{sec:equations} on equations. However, if I move that section, I'll still be referring to the right section.
%Better explained by Figure\footnote{a- Don't use footnotes.\ b- Capitalize the word ``Figure''}~\ref{fig:example}.  You can keep all of your figures in a sub-directory such as \lq\lq{}pix\rq\rq{}, which is used in this template.
%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=.5\textwidth]{WSUThesisTemplate/pix/example.pdf} %
%   \caption{Example caption.}
%\label{fig:example}
%\end{figure}
%
%The width of the figure can be set based on percentage of text width as set in Figure~\ref{fig:example} or based on inches as used in Figure~\ref{fig:example2}.  Also notice the \verb'\label' is after the \verb'\caption'.  This must be true, or the hyperlinks and figure numbers will not be correct.
%
%%----------------------------------------------------------
%\begin{figure}[!t] %forced to top of page so no text will be stranded above
%   \centering
%   \includegraphics[width=3in]{WSUThesisTemplate/pix/example.pdf} %
%   \caption[Short Figure Caption]{Example caption that is way too long for the list of figures, is a run on sentence, has no purpose being this long, except to show you how to avoid such a crazy long entry in your list of figures.}
%\label{fig:example2}
%\end{figure}
%%----------------------------------------------------------
%
%Don't ask me why the label command has to come late in a figure. It does.  Remember, color won't print well in black and white. Use dashes and dash-dots, etc, for hard copies. I'll document a trick for this later. Basically, make two graphics directories, one for color, one for black and white. Then, use the \textsf{graphicspath} command to choose the one you want. You can Google this for now.
%
%%=============================
%\section{Sub-figures}
%%=============================
%You can also make sub-figures and reference each of them individually.  You can reference the entire Figure~\ref{fig:1x2_subfigs}, or just Figure~\ref{fig:example_a} or Figure~\ref{fig:example_b}.
%%-----------------------------------------------------------------
%\begin{figure}[!ht]
%  \centering
%  \subfigure[First sub-figure]{
%\label{fig:example_a}
%   \includegraphics[width=.45\textwidth]{WSUThesisTemplate/pix/example.pdf}}
%   \subfigure[Second sub-figure]{
%\label{fig:example_b}
%   \includegraphics[width=.45\textwidth]{WSUThesisTemplate/pix/example.pdf}}
%   \caption{Side-by-side sub-figures.}
%\label{fig:1x2_subfigs}
%\end{figure}
%%-----------------------------------------------------------------
%
%
%If you want 4 total figures, just add a line break, \verb'\\', after the second sub-figure as shown in Figure~\ref{fig:2x2_subfigs}.  You can add spacing between them with the \verb'\quad' or \verb'\qquad' commands.  There is more space between Figures~\ref{fig:example_2x2a} and~\ref{fig:example_2x2b} to show the use of this spacing.  Make sure all of your spacing is equal.  And don't make your figures too small.  As my advisor told me, ``old people read these''\cite{Mark}.
%
%%-----------------------------------------------------------------
%\begin{figure}[!t]
%   \centering
%   \subfigure[First sub-figure]{
%\label{fig:example_2x2a}
%   \includegraphics[width=.25\textwidth]{WSUThesisTemplate/pix/example.pdf}} \quad
%   \subfigure[Second sub-figure]{
%\label{fig:example_2x2b}
%   \includegraphics[width=.25\textwidth]{WSUThesisTemplate/pix/example.pdf}}\\
%      \subfigure[Third sub-figure]{
%\label{fig:example_2x2c}
%   \includegraphics[width=.25\textwidth]{WSUThesisTemplate/pix/example.pdf}}
%   \subfigure[Fourth sub-figure]{
%\label{fig:example_2x2d}
%   \includegraphics[width=.25\textwidth]{WSUThesisTemplate/pix/example.pdf}}
%\caption{2$\times$2 sub-figures.}
%\label{fig:2x2_subfigs}
%\end{figure}
%%-----------------------------------------------------------------
%
%%=============================
%\section{Including Chapters or Files}
%%=============================
%You can include chapters using the \verb'\include' command. See the \LaTeX\ file.  Each file can be included separately as to keep editing localized to each chapter.
%% ------------------------------------------------------------------------
%%\include{chapter2}
%% ------------------------------------------------------------------------
%%\include{chapter3}
%% ------------------------------------------------------------------------
%
%You can also use the \verb'\input' command to include items without forcing a page break.  This becomes handy when generating a table, you can leave the reference in the main document and the table can be updated separately.
%\input{WSUThesisTemplate/table_bench_tests}
%
%Using the \verb'booktabs' package makes very professional looking tables by varying the thickness of the lines which can be customized.
%
%%=============================
%\clearpage \section{Inserting Code}
%%=============================
%If you want to insert code into your document by reference, instead of copy/paste, you even use the \href{https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings}{listings} package.  
%
%\href{http://www.mathworks.com}{Matlab} users may find the simpler interface of the \verb'mcode' package easier.  To do that simply comment the \verb'\usepackage{listings}' line near the top of this (the \verb'.tex') document and uncomment the preceding line. You can choose to between several options to frame, have numbered lines, automatic line breaks and more.  Below is an example of listing a MATLAB\textsuperscript{\textregistered} m-file.
%
%\lstinputlisting[language=matlab, basicstyle=\linespread{1}\normalsize]{WSUThesisTemplate/importfile.m}
%
%%=============================
%\chapter[Programs]{Typesetting Programs using \LaTeX\ }
%%=============================
%\section{Windows}
%%=============================
%Below are some programs for Windows:
%\begin{itemize}
%\item \href{http://miktex.org/}{MiK\TeX}
%\subitem- Up-to-date implementation of \TeX\
%\subitem- Side-by-side comparison of source and PDF
%\subitem- Has portable version that can be run from portable storage device
%\item \href{http://www.lyx.org/}{LyX}
%\subitem- Graphical interface used with \TeX\ and \LaTeX\
%\item \href{http://www.tug.org/texlive/}{\TeX Live} (also Unix)
%\item \href{http://www.tug.org/protext/}{pro\TeX t}
%\end{itemize}
%
%%=============================
%\section{Mac OS}
%%=============================
%Below are some programs for Mac OS:\@
%\begin{itemize}
%\item \href{http://www.lyx.org/}{LyX}
%\item\href{http://www.tug.org/mactex/}{Mac\TeX}
%\subitem- \TeX\ Live with the addition of Mac specific programs
%\item gw\TeX\   (Mac OS X)
%\item Latexian (Mac OS X)
%\end{itemize}
%
%\section{References}
%\label{sec:refs}
%\begin{itemize}
%\item \href{http://www.ctan.org}{CTAN} home page
%\item \href{http://en.wikibooks.org/wiki/LaTeX/}{Wikibooks} \LaTeX\ home page
%\end{itemize}


%
%
%-----------------------------------------------------------------------
% Bibliography
%-----------------------------------------------------------------------
\clearpage \phantomsection\ %used to correctly anchor hyperlinks, just
                           %trust us and leave it alone.
\renewcommand\baselinestretch{1.0}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{references}
%
%-----------------------------------------------------------------------
% Appendices
%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%
\begin{appendices}
\phantomsection\ %use \phantomsection command to correctly anchor hyperlinks
\include{AppendixA}
\phantomsection\
\include{AppendixB}
\end{appendices}
%%%%%%%%%%%%%%%%%%%
%
%
%
% End of document
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "WSUThesisTemplate"
%%% End:
